"""
–°–µ—Ä–≤–∏—Å –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫ (—Å–∞–π—Ç—ã + Telegram –∫–∞–Ω–∞–ª—ã)
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ —Å—Å—ã–ª–æ–∫ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ AI
"""

import asyncio
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from urllib.parse import urlparse
import aiohttp
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)


class LinkAnalysisService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""

    def __init__(self):
        self.website_cache = {}  # –ö–µ—à –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–æ–≤ (URL -> {content, timestamp})
        self.cache_ttl = timedelta(hours=24)  # TTL –¥–ª—è –∫–µ—à–∞ - 24 —á–∞—Å–∞
        self.request_timeout = 5  # –¢–∞–π–º–∞—É—Ç –¥–ª—è HTTP –∑–∞–ø—Ä–æ—Å–æ–≤ - 5 —Å–µ–∫—É–Ω–¥

    async def analyze_links(
        self,
        website_url: Optional[str] = None,
        telegram_links: List[str] = None
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Å—ã–ª–∫–∏ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

        Args:
            website_url: URL —Å–∞–π—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            telegram_links: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ Telegram –∫–∞–Ω–∞–ª—ã

        Returns:
            Dict —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑: —Å–∞–π—Ç={website_url}, telegram={len(telegram_links or [])} –∫–∞–Ω–∞–ª–æ–≤")

            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            website_content = None
            telegram_content = []

            # 1. –ü–∞—Ä—Å–∏–º —Å–∞–π—Ç (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
            if website_url:
                website_content = await self._parse_website(website_url)
                logger.info(f"‚úÖ –°–∞–π—Ç —Å–ø–∞—Ä—Å–µ–Ω: {len(website_content.get('text', ''))} —Å–∏–º–≤–æ–ª–æ–≤")

            # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º Telegram –∫–∞–Ω–∞–ª—ã (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
            if telegram_links:
                telegram_content = await self._analyze_telegram_channels(telegram_links)
                logger.info(f"‚úÖ Telegram –∫–∞–Ω–∞–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(telegram_content)}")

            # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ AI
            analysis_result = await self._generate_ai_recommendations(
                website_content=website_content,
                telegram_content=telegram_content
            )

            logger.info(f"‚úÖ AI-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∏—à–∞={analysis_result.get('suggestedNiche')}")

            return {
                "analysis": analysis_result
            }

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫: {e}", exc_info=True)
            raise

    async def _parse_website(self, url: str) -> Optional[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç —Å–∞–π—Ç —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ç–∞–π–º–∞—É—Ç–æ–º

        Args:
            url: URL —Å–∞–π—Ç–∞

        Returns:
            Dict —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º —Å–∞–π—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            if url in self.website_cache:
                cached = self.website_cache[url]
                cache_age = datetime.now() - cached['timestamp']
                if cache_age < self.cache_ttl:
                    logger.info(f"üì¶ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞ (–≤–æ–∑—Ä–∞—Å—Ç: {cache_age})")
                    return cached['content']

            logger.info(f"üåê –ü–∞—Ä—Å–∏–º —Å–∞–π—Ç: {url}")

            # –î–µ–ª–∞–µ–º HTTP –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            timeout = aiohttp.ClientTimeout(total=self.request_timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                async with session.get(url, headers=headers) as response:
                    if response.status != 200:
                        logger.warning(f"‚ö†Ô∏è –°–∞–π—Ç –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status}")
                        return None

                    html = await response.text()

            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(html, 'html.parser')

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(['script', 'style', 'nav', 'footer', 'header']):
                script.decompose()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = soup.get_text(separator=' ', strip=True)

            # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            text = re.sub(r'\s+', ' ', text).strip()

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É (–ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤)
            text = text[:3000] if len(text) > 3000 else text

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞—Ç–µ–≥–∏
            title = soup.title.string if soup.title else None
            meta_description = None
            meta_keywords = None

            for meta in soup.find_all('meta'):
                if meta.get('name') == 'description':
                    meta_description = meta.get('content')
                elif meta.get('name') == 'keywords':
                    meta_keywords = meta.get('content')

            content = {
                'url': url,
                'title': title,
                'description': meta_description,
                'keywords': meta_keywords,
                'text': text
            }

            # –ö–µ—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.website_cache[url] = {
                'content': content,
                'timestamp': datetime.now()
            }

            return content

        except asyncio.TimeoutError:
            logger.warning(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å–∞–π—Ç–∞: {url}")
            return None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞ {url}: {e}")
            return None

    async def _analyze_telegram_channels(self, telegram_links: List[str]) -> List[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Telegram –∫–∞–Ω–∞–ª—ã

        Args:
            telegram_links: –°–ø–∏—Å–æ–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–∞–Ω–∞–ª—ã

        Returns:
            –°–ø–∏—Å–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–∞–Ω–∞–ª–∞—Ö
        """
        results = []

        for link in telegram_links:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º username –∫–∞–Ω–∞–ª–∞ –∏–∑ —Å—Å—ã–ª–∫–∏
                username = self._extract_telegram_username(link)
                if not username:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å username –∏–∑: {link}")
                    continue

                # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ TelegramMCP
                # –ù–æ –¥–ª—è –ø–µ—Ä–≤–æ–π –≤–µ—Ä—Å–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                results.append({
                    'username': username,
                    'link': link,
                    'accessible': False  # –ü–æ–∫–∞ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
                })

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ Telegram –∫–∞–Ω–∞–ª–∞ {link}: {e}")
                continue

        return results

    def _extract_telegram_username(self, link: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç username –∏–∑ Telegram —Å—Å—ã–ª–∫–∏"""
        # t.me/channel –∏–ª–∏ @channel –∏–ª–∏ https://t.me/channel
        if link.startswith('@'):
            return link[1:]

        # –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è username
        match = re.search(r't\.me/([a-zA-Z0-9_]+)', link)
        if match:
            return match.group(1)

        return None

    async def _generate_ai_recommendations(
        self,
        website_content: Optional[Dict[str, Any]] = None,
        telegram_content: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ AI –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            website_content: –ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞
            telegram_content: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Telegram –∫–∞–Ω–∞–ª–∞—Ö

        Returns:
            Dict —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è AI
            prompt_parts = []

            prompt_parts.append(
                "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –±–∏–∑–Ω–µ—Å–µ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n"
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–π—Ç–µ
            if website_content:
                prompt_parts.append("\n=== –°–ê–ô–¢ ===")
                if website_content.get('title'):
                    prompt_parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {website_content['title']}")
                if website_content.get('description'):
                    prompt_parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {website_content['description']}")
                if website_content.get('keywords'):
                    prompt_parts.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {website_content['keywords']}")
                if website_content.get('text'):
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞
                    text_preview = website_content['text'][:1000]
                    prompt_parts.append(f"–ö–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞: {text_preview}")

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Telegram –∫–∞–Ω–∞–ª–∞—Ö
            if telegram_content and len(telegram_content) > 0:
                prompt_parts.append("\n=== TELEGRAM –ö–ê–ù–ê–õ–´ ===")
                for channel in telegram_content:
                    prompt_parts.append(f"–ö–∞–Ω–∞–ª: {channel.get('username', 'N/A')}")

            prompt_parts.append(
                "\n=== –ó–ê–î–ê–ß–ê ===\n"
                "–ù–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–∏:\n"
                "1. –ù–∏—à—É –±–∏–∑–Ω–µ—Å–∞ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ)\n"
                "2. –¶–µ–ª–µ–≤—É—é –∞—É–¥–∏—Ç–æ—Ä–∏—é (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫—Ç–æ –∏–º–µ–Ω–Ω–æ)\n"
                "3. –¢–∏–ø—ã –±–∏–∑–Ω–µ—Å–∞ (–≤—ã–±–µ—Ä–∏ –∏–∑: product, service, education, consulting)\n"
                "4. –ë–∏–∑–Ω–µ—Å-—Ü–µ–ª–∏ (–≤—ã–±–µ—Ä–∏ –∏–∑: creating_posts, engagement, lead_processing, sales, growth)\n"
                "5. –ü—Ä–∏–∑—ã–≤—ã –∫ –¥–µ–π—Å—Ç–≤–∏—é (–≤—ã–±–µ—Ä–∏ –∏–∑: consultation, purchase, subscribe, read_more)\n"
                "6. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–≤—ã–±–µ—Ä–∏ –∏–∑: professional, friendly, casual, expert)\n"
                "7. –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)\n\n"
                "–û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:\n"
                "{\n"
                '  "niche": "...",\n'
                '  "audience": "...",\n'
                '  "businessTypes": ["..."],\n'
                '  "goals": ["..."],\n'
                '  "cta": ["..."],\n'
                '  "tone": "...",\n'
                '  "reasoning": "..."\n'
                "}"
            )

            prompt = "\n".join(prompt_parts)

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Vertex AI
            from app.mcp.integrations.vertex_ai import VertexAIMCP
            from app.mcp.config import is_mcp_enabled

            if is_mcp_enabled('vertex_ai'):
                logger.info("ü§ñ Vertex AI –¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–∑—ã–≤–∞–µ–º generate_content")
                vertex_ai = VertexAIMCP()
                response = await vertex_ai.generate_content(
                    prompt=prompt,
                    temperature=0.7,
                    max_tokens=1000
                )

                logger.info(f"üîç Vertex AI –æ—Ç–≤–µ—Ç: success={response.success}, data={bool(response.data)}")

                if response.success and response.data:
                    import json
                    ai_text = response.data.get('generated_text', '{}')
                    logger.info(f"üìù AI —Ç–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω: {len(ai_text)} —Å–∏–º–≤–æ–ª–æ–≤")

                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                    try:
                        # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–µ–∂–¥—É { –∏ })
                        json_match = re.search(r'\{.*\}', ai_text, re.DOTALL)
                        if json_match:
                            ai_data = json.loads(json_match.group(0))

                            logger.info(f"‚úÖ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã: {ai_data.get('niche')}")
                            return {
                                'suggestedNiche': ai_data.get('niche'),
                                'suggestedAudience': ai_data.get('audience'),
                                'suggestedBusinessTypes': ai_data.get('businessTypes', []),
                                'suggestedGoals': ai_data.get('goals', []),
                                'suggestedCta': ai_data.get('cta', []),
                                'tone': ai_data.get('tone'),
                                'reasoning': ai_data.get('reasoning')
                            }
                        else:
                            logger.warning(f"‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ AI. –û—Ç–≤–µ—Ç: {ai_text[:200]}")
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ AI –æ—Ç–≤–µ—Ç–∞: {e}")
                        logger.warning(f"AI —Ç–µ–∫—Å—Ç –±—ã–ª: {ai_text[:200]}")
                else:
                    logger.warning(f"‚ö†Ô∏è Vertex AI –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É –∏–ª–∏ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                    if hasattr(response, 'error'):
                        logger.error(f"–û—à–∏–±–∫–∞ Vertex AI: {response.error}")
            else:
                logger.warning("‚ö†Ô∏è Vertex AI –Ω–µ –≤–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            logger.warning("‚ö†Ô∏è AI –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            return self._get_fallback_recommendations(website_content)

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}", exc_info=True)
            return self._get_fallback_recommendations(website_content)

    def _get_fallback_recommendations(
        self,
        website_content: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±–µ–∑ AI"""

        niche = "–û–Ω–ª–∞–π–Ω-–±–∏–∑–Ω–µ—Å"
        audience = "–®–∏—Ä–æ–∫–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è, –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–∞—è –≤ –≤–∞—à–µ–º –ø—Ä–æ–¥—É–∫—Ç–µ –∏–ª–∏ —É—Å–ª—É–≥–µ"

        # –ï—Å–ª–∏ –µ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å–∞–π—Ç–∞ - –ø—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if website_content:
            title = website_content.get('title', '')
            description = website_content.get('description', '')
            text = website_content.get('text', '')

            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∏—à–∏
            combined_text = f"{title} {description} {text}".lower()

            if any(word in combined_text for word in ['–≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏–µ', '–∫–∞–º–µ—Ä', '–æ—Ö—Ä–∞–Ω']):
                niche = "–°–∏—Å—Ç–µ–º—ã –≤–∏–¥–µ–æ–Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
                audience = "–í–ª–∞–¥–µ–ª—å—Ü—ã —á–∞—Å—Ç–Ω—ã—Ö –¥–æ–º–æ–≤ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏"
            elif any(word in combined_text for word in ['–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ', '–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å']):
                niche = "–û–Ω–ª–∞–π–Ω-–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ"
                audience = "–õ—é–¥–∏, –∂–µ–ª–∞—é—â–∏–µ –ø–æ–ª—É—á–∏—Ç—å –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∏ –Ω–∞–≤—ã–∫–∏"
            elif any(word in combined_text for word in ['–∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü', '—É—Å–ª—É–≥', '—Å–µ—Ä–≤–∏—Å']):
                niche = "–ö–æ–Ω—Å–∞–ª—Ç–∏–Ω–≥–æ–≤—ã–µ —É—Å–ª—É–≥–∏"
                audience = "–ë–∏–∑–Ω–µ—Å –∏ —á–∞—Å—Ç–Ω—ã–µ –ª–∏—Ü–∞, –Ω—É–∂–¥–∞—é—â–∏–µ—Å—è –≤ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏"

        return {
            'suggestedNiche': niche,
            'suggestedAudience': audience,
            'suggestedBusinessTypes': ['service'],
            'suggestedGoals': ['creating_posts', 'lead_processing'],
            'suggestedCta': ['consultation'],
            'tone': 'professional',
            'reasoning': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –±–∞–∑–æ–≤–æ–º –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ç—Ä–µ–±—É–µ—Ç—Å—è AI-–∞–Ω–∞–ª–∏–∑.'
        }
