"""
ResearchFactCheckAgent - MVP –≤–µ—Ä—Å–∏—è –∞–≥–µ–Ω—Ç–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤
–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏
"""

import asyncio
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from enum import Enum

from ..orchestrator.agent_manager import BaseAgent, AgentCapability
from ..orchestrator.workflow_engine import Task, TaskType, TaskPriority
from ..mcp.integrations.news import NewsMCP
from ..mcp.integrations.wikipedia import WikipediaMCP
from ..mcp.integrations.vertex_ai import VertexAIMCP
from ..mcp.config import get_mcp_config, is_mcp_enabled

logger = logging.getLogger(__name__)


class FactCheckStatus(Enum):
    """–°—Ç–∞—Ç—É—Å—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤"""
    VERIFIED = "verified"
    PARTIALLY_VERIFIED = "partially_verified"
    UNVERIFIED = "unverified"
    DISPUTED = "disputed"
    FALSE = "false"


class ClaimType(Enum):
    """–¢–∏–ø—ã —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
    STATISTICAL = "statistical"
    TEMPORAL = "temporal"
    QUOTE = "quote"
    SCIENTIFIC = "scientific"
    HISTORICAL = "historical"
    GENERAL = "general"


@dataclass
class FactCheckResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–∞"""
    claim: str
    claim_type: ClaimType
    status: FactCheckStatus
    confidence_score: float  # 0.0 - 1.0
    verification_sources: List[str]
    evidence: List[str]
    recommendations: List[str]
    checked_at: datetime = field(default_factory=datetime.now)


@dataclass
class ContentFactCheckReport:
    """–û—Ç—á–µ—Ç –æ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    content_id: str
    total_claims: int
    verified_claims: int
    disputed_claims: int
    false_claims: int
    overall_confidence: float
    fact_check_results: List[FactCheckResult]
    recommendations: List[str]
    generated_at: datetime = field(default_factory=datetime.now)


class ResearchFactCheckAgent(BaseAgent):
    """MVP –∞–≥–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤ –∏ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
    
    def __init__(self, agent_id: str = "research_factcheck_agent"):
        capability = AgentCapability(
            task_types=[TaskType.PLANNED, TaskType.COMPLEX],
            max_concurrent_tasks=2,  # –§–∞–∫—Ç—á–µ–∫–∏–Ω–≥ —Ç—Ä–µ–±—É–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
            specializations=["fact_checking", "research", "verification", "content_analysis"],
            performance_score=0.9  # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ç–æ—á–Ω–µ–µ
        )
        super().__init__(agent_id, "Research & FactCheck Agent (MVP)", capability)
        
        # MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        self.news_mcp = None
        self.wikipedia_mcp = None
        self.vertex_mcp = None  # Vertex AI –¥–ª—è —Ñ–∞–∫—Ç—á–µ–∫–∞ —Å Grounding
        
        # –ö—ç—à –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ (–≤ –ø–∞–º—è—Ç–∏)
        self.fact_cache = {}
        self.cache_ttl = timedelta(hours=24)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        self.claim_patterns = self._load_claim_patterns()
        
        # –û—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        self.source_reliability = self._load_source_reliability()
        
        self._initialize_mcp_integrations()
        logger.info(f"ResearchFactCheckAgent MVP {agent_id} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def can_handle_task(self, task: Task) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ ResearchFactCheckAgent –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–¥–∞—á—É
        –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –∏ –∑–∞–¥–∞—á–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è
        if not super().can_handle_task(task):
            return False
        
        # ResearchFactCheckAgent –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
        if "Publish" in task.name or "publish" in task.name.lower():
            return False
        
        # ResearchFactCheckAgent –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏/–ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        # –≠—Ç–æ –¥–æ–ª–∂–Ω—ã –¥–µ–ª–∞—Ç—å MultimediaProducerAgent
        image_keywords = ["Image", "image", "Stock", "stock", "Generate", "generate", "multimedia"]
        if any(keyword in task.name for keyword in image_keywords):
            return False
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏
        task_context = task.context if hasattr(task, 'context') else {}
        if task_context.get("image_source") or task_context.get("content_type") in ["post_image", "image"]:
            return False
        
        return True
    
    def _load_claim_patterns(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
        return {
            "statistical": r'\d+(?:\.\d+)?%?',
            "temporal": r'\d{1,2}[./]\d{1,2}[./]\d{2,4}|\d{4}',
            "quote": r'"[^"]*"',
            "scientific": r'(?:–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ|—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç|–∞–Ω–∞–ª–∏–∑|–¥–∞–Ω–Ω—ã–µ|—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã|—É—á–µ–Ω—ã–µ|–Ω–∞—É–∫–∞)',
            "historical": r'(?:–≤ \d{4}|–≤ –ø—Ä–æ—à–ª–æ–º|–∏—Å—Ç–æ—Ä–∏—è|–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π)',
            "general": r'(?:—É—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç|–≥–æ–≤–æ—Ä–∏—Ç|—Å—á–∏—Ç–∞–µ—Ç|–ø–æ–ª–∞–≥–∞–µ—Ç)'
        }
    
    def _load_source_reliability(self) -> Dict[str, float]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ—Ü–µ–Ω–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        return {
            # Wikipedia (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—Ç–∞—Ç—å–∏)
            "wikipedia": 0.75,
            "ru.wikipedia": 0.70,
            "en.wikipedia": 0.80,
            
            # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            "bbc.com": 0.87,
            "reuters.com": 0.89,
            "tass.ru": 0.85,
            "ria.ru": 0.83,
            "lenta.ru": 0.80,
            "meduza.io": 0.82,
            
            # –ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
            "nature.com": 0.95,
            "science.org": 0.95,
            "pubmed.ncbi.nlm.nih.gov": 0.90,
            
            # –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            "rosstat.gov.ru": 0.90,
            "gov.ru": 0.85,
            "kremlin.ru": 0.80
        }
    
    def _initialize_mcp_integrations(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        try:
            # Vertex AI –¥–ª—è —Ñ–∞–∫—Ç—á–µ–∫–∞ —Å Grounding (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥)
            if is_mcp_enabled('vertex_ai'):
                try:
                    self.vertex_mcp = VertexAIMCP()
                    logger.info("VertexAIMCP –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ ResearchFactCheckAgent")
                except Exception as e:
                    logger.warning(f"VertexAIMCP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e} - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è fallback")
                    self.vertex_mcp = None
            else:
                logger.warning("Vertex AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è fallback")
            
            # News API (fallback)
            if is_mcp_enabled('news'):
                self.news_mcp = NewsMCP()
                logger.info("NewsMCP –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ ResearchFactCheckAgent")
            else:
                logger.warning("NewsMCP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è fallback")
            
            # Wikipedia API (fallback)
            if is_mcp_enabled('wikipedia'):
                self.wikipedia_mcp = WikipediaMCP()
                logger.info("WikipediaMCP –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤ ResearchFactCheckAgent")
            else:
                logger.warning("WikipediaMCP –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è fallback")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MCP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π: {e}")
            self.news_mcp = None
            self.wikipedia_mcp = None
            self.vertex_mcp = None
    
    async def execute_task(self, task: Task) -> Dict[str, Any]:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤"""
        try:
            logger.info(f"ResearchFactCheckAgent –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É: {task.name}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            content_data = task.context.get("content", {})
            check_type = task.context.get("check_type", "basic")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∞–∫—Ç–æ–≤
            report = await self._fact_check_content(content_data, check_type)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "task_id": task.id,
                "agent_id": self.agent_id,
                "fact_check_report": {
                    "content_id": report.content_id,
                    "total_claims": report.total_claims,
                    "verified_claims": report.verified_claims,
                    "disputed_claims": report.disputed_claims,
                    "false_claims": report.false_claims,
                    "overall_confidence": report.overall_confidence,
                    "recommendations": report.recommendations
                },
                "detailed_results": [
                    {
                        "claim": result.claim,
                        "type": result.claim_type.value,
                        "status": result.status.value,
                        "confidence": result.confidence_score,
                        "sources": result.verification_sources,
                        "evidence": result.evidence,
                        "recommendations": result.recommendations
                    }
                    for result in report.fact_check_results
                ],
                "status": "completed",
                "timestamp": datetime.now().isoformat()
            }
            
            logger.info(f"ResearchFactCheckAgent –∑–∞–≤–µ—Ä—à–∏–ª –∑–∞–¥–∞—á—É {task.id}")
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ ResearchFactCheckAgent: {e}")
            raise
    
    async def _fact_check_content(self, content_data: Dict[str, Any], 
                                check_type: str) -> ContentFactCheckReport:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫—É —Ñ–∞–∫—Ç–æ–≤ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        text = content_data.get("text", "")
        content_id = content_data.get("id", "")
        
        if not text.strip():
            logger.warning("–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤")
            return ContentFactCheckReport(
                content_id=content_id,
                total_claims=0,
                verified_claims=0,
                disputed_claims=0,
                false_claims=0,
                overall_confidence=0.0,
                fact_check_results=[],
                recommendations=["–¢–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ–≤ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞"]
            )
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        claims = await self._extract_claims(text)
        logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(claims)} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        fact_check_results = []
        for claim in claims:
            result = await self._verify_claim(claim, check_type)
            fact_check_results.append(result)
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_claims = len(fact_check_results)
        verified_claims = sum(1 for r in fact_check_results 
                            if r.status == FactCheckStatus.VERIFIED)
        disputed_claims = sum(1 for r in fact_check_results 
                            if r.status == FactCheckStatus.DISPUTED)
        false_claims = sum(1 for r in fact_check_results 
                         if r.status == FactCheckStatus.FALSE)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        overall_confidence = sum(r.confidence_score for r in fact_check_results) / total_claims if total_claims > 0 else 0.0
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = await self._generate_recommendations(fact_check_results)
        
        return ContentFactCheckReport(
            content_id=content_id,
            total_claims=total_claims,
            verified_claims=verified_claims,
            disputed_claims=disputed_claims,
            false_claims=false_claims,
            overall_confidence=overall_confidence,
            fact_check_results=fact_check_results,
            recommendations=recommendations
        )
    
    async def _extract_claims(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        claims = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for claim_type, pattern in self.claim_patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                context_claim = self._extract_context_around_match(text, match, 50)
                if context_claim and len(context_claim.strip()) > 10:
                    claims.append(context_claim.strip())
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        unique_claims = []
        for claim in claims:
            if (len(claim) > 15 and 
                claim not in unique_claims and 
                not any(claim in existing for existing in unique_claims)):
                unique_claims.append(claim)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –¥–ª—è MVP
        return unique_claims[:10]
    
    def _extract_context_around_match(self, text: str, match: str, context_size: int) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        try:
            match_index = text.lower().find(match.lower())
            if match_index == -1:
                return ""
            
            start = max(0, match_index - context_size)
            end = min(len(text), match_index + len(match) + context_size)
            
            context = text[start:end]
            
            # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            sentences = re.split(r'[.!?]', context)
            if len(sentences) > 1:
                # –ë–µ—Ä–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, —Å–æ–¥–µ—Ä–∂–∞—â–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                for sentence in sentences:
                    if match.lower() in sentence.lower():
                        return sentence.strip()
            
            return context.strip()
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return ""
    
    async def _verify_claim(self, claim: str, check_type: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        claim_type = await self._classify_claim(claim)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –∫—ç—à–µ
        cache_key = f"{claim}_{check_type}"
        if cache_key in self.fact_cache:
            cached_result = self.fact_cache[cache_key]
            if datetime.now() - cached_result.checked_at < self.cache_ttl:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è: {claim[:50]}...")
                return cached_result
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∏—Å–ø–æ–ª—å–∑—É–µ–º Vertex AI —Å Grounding –¥–ª—è —Ñ–∞–∫—Ç—á–µ–∫–∞
        if self.vertex_mcp:
            try:
                logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º Vertex AI Grounding –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏: {claim[:50]}...")
                result = await self._verify_claim_with_vertex(claim, claim_type)
                
                # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                self.fact_cache[cache_key] = result
                return result
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —á–µ—Ä–µ–∑ Vertex AI: {e} - –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        
        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç–æ–¥—ã –ø—Ä–æ–≤–µ—Ä–∫–∏
        if claim_type == ClaimType.STATISTICAL:
            result = await self._verify_statistical_claim(claim)
        elif claim_type == ClaimType.TEMPORAL:
            result = await self._verify_temporal_claim(claim)
        elif claim_type == ClaimType.QUOTE:
            result = await self._verify_quote_claim(claim)
        elif claim_type == ClaimType.SCIENTIFIC:
            result = await self._verify_scientific_claim(claim)
        else:
            result = await self._verify_general_claim(claim)
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.fact_cache[cache_key] = result
        
        return result
    
    async def _verify_claim_with_vertex(self, claim: str, claim_type: ClaimType) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Vertex AI —Å Grounding"""
        try:
            # –í—ã–∑—ã–≤–∞–µ–º fact_check —á–µ—Ä–µ–∑ Vertex AI
            response = await self.vertex_mcp.fact_check(claim=claim, context=None)
            
            if not response.success:
                logger.warning(f"Vertex AI fact_check –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.error}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                return FactCheckResult(
                    claim=claim,
                    claim_type=claim_type,
                    status=FactCheckStatus.UNVERIFIED,
                    confidence_score=0.0,
                    verification_sources=[],
                    evidence=[],
                    recommendations=["–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á–µ—Ä–µ–∑ Vertex AI"]
                )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç –æ—Ç Gemini
            generated_text = response.data.get("generated_text", "")
            metadata = response.metadata or {}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—Ç–≤–µ—Ç–∞
            verdict = self._parse_vertex_verdict(generated_text)
            confidence_score = self._calculate_confidence_from_vertex(verdict, metadata)
            sources = self._extract_sources_from_vertex(generated_text, metadata)
            evidence = self._extract_evidence_from_vertex(generated_text)
            recommendations = self._generate_recommendations_from_vertex(verdict, generated_text)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–¥–∏–∫—Ç–∞
            status = self._map_verdict_to_status(verdict)
            
            logger.info(f"Vertex AI –ø—Ä–æ–≤–µ—Ä–∏–ª —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: {verdict}, confidence: {confidence_score}")
            
            return FactCheckResult(
                claim=claim,
                claim_type=claim_type,
                status=status,
                confidence_score=confidence_score,
                verification_sources=sources,
                evidence=evidence,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ _verify_claim_with_vertex: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –æ—à–∏–±–∫–æ–π
            return FactCheckResult(
                claim=claim,
                claim_type=claim_type,
                status=FactCheckStatus.UNVERIFIED,
                confidence_score=0.0,
                verification_sources=[],
                evidence=[],
                recommendations=[f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —á–µ—Ä–µ–∑ Vertex AI: {str(e)}"]
            )
    
    def _parse_vertex_verdict(self, text: str) -> str:
        """–ü–∞—Ä—Å–∏—Ç –≤–µ—Ä–¥–∏–∫—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞ Vertex AI"""
        text_lower = text.lower()
        
        if "–ø—Ä–∞–≤–¥–∞" in text_lower and "–ª–æ–∂—å" not in text_lower:
            return "–ü—Ä–∞–≤–¥–∞"
        elif "–ª–æ–∂—å" in text_lower or "–Ω–µ–≤–µ—Ä–Ω–æ" in text_lower or "–æ—à–∏–±–æ—á–Ω–æ" in text_lower:
            return "–õ–æ–∂—å"
        elif "—á–∞—Å—Ç–∏—á–Ω–æ" in text_lower or "—á–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–¥–∞" in text_lower:
            return "–ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–¥–∞"
        elif "–Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å" in text_lower or "–Ω–µ —É–¥–∞–ª–æ—Å—å" in text_lower:
            return "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å"
        else:
            return "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ"
    
    def _calculate_confidence_from_vertex(self, verdict: str, metadata: Dict[str, Any]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç confidence score –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–¥–∏–∫—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        base_confidence = {
            "–ü—Ä–∞–≤–¥–∞": 0.9,
            "–ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–¥–∞": 0.6,
            "–õ–æ–∂—å": 0.1,
            "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å": 0.3,
            "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ": 0.5
        }.get(verdict, 0.5)
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º confidence –µ—Å–ª–∏ –µ—Å—Ç—å grounding sources
        if metadata.get("grounding_sources", 0) > 0:
            base_confidence = min(1.0, base_confidence + 0.1)
        
        return base_confidence
    
    def _extract_sources_from_vertex(self, text: str, metadata: Dict[str, Any]) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Vertex AI"""
        sources = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏—é "–ò–°–¢–û–ß–ù–ò–ö–ò" –≤ —Ç–µ–∫—Å—Ç–µ
        if "–∏—Å—Ç–æ—á–Ω–∏–∫–∏:" in text.lower():
            sources_section = text.lower().split("–∏—Å—Ç–æ—á–Ω–∏–∫–∏:")[-1]
            # –ò–∑–≤–ª–µ–∫–∞–µ–º URL –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            import re
            urls = re.findall(r'https?://[^\s]+', sources_section)
            sources.extend(urls)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ grounding sources –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if metadata.get("grounding_sources", 0) > 0:
            sources.append(f"Google Search Grounding ({metadata['grounding_sources']} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)")
        
        return sources if sources else ["Google Search Grounding"]
    
    def _extract_evidence_from_vertex(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Vertex AI"""
        evidence = []
        
        # –ò—â–µ–º —Å–µ–∫—Ü–∏—é "–û–ë–™–Ø–°–ù–ï–ù–ò–ï" –≤ —Ç–µ–∫—Å—Ç–µ
        if "–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:" in text.lower():
            explanation_section = text.lower().split("–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:")[-1].split("–∏—Å—Ç–æ—á–Ω–∏–∫–∏:")[0]
            evidence.append(explanation_section.strip())
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–π —Å–µ–∫—Ü–∏–∏, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ
        if not evidence:
            evidence.append(text[:500])  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤
        
        return evidence
    
    def _generate_recommendations_from_vertex(self, verdict: str, text: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Ä–¥–∏–∫—Ç–∞ Vertex AI"""
        recommendations = []
        
        if verdict == "–ü—Ä–∞–≤–¥–∞":
            recommendations.append("‚úÖ –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ —á–µ—Ä–µ–∑ Google Search Grounding")
        elif verdict == "–ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–¥–∞":
            recommendations.append("‚ö†Ô∏è –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–∏—Ç—å –¥–µ—Ç–∞–ª–∏")
        elif verdict == "–õ–æ–∂—å":
            recommendations.append("‚ùå –£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–æ - —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        elif verdict == "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å":
            recommendations.append("‚ÑπÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        else:
            recommendations.append("‚ö†Ô∏è –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
        
        return recommendations
    
    def _map_verdict_to_status(self, verdict: str) -> FactCheckStatus:
        """–ú–∞–ø–ø–∏—Ç –≤–µ—Ä–¥–∏–∫—Ç Vertex AI –≤ FactCheckStatus"""
        mapping = {
            "–ü—Ä–∞–≤–¥–∞": FactCheckStatus.VERIFIED,
            "–ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–¥–∞": FactCheckStatus.PARTIALLY_VERIFIED,
            "–õ–æ–∂—å": FactCheckStatus.FALSE,
            "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å": FactCheckStatus.UNVERIFIED,
            "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ": FactCheckStatus.UNVERIFIED
        }
        return mapping.get(verdict, FactCheckStatus.UNVERIFIED)
    
    async def _classify_claim(self, claim: str) -> ClaimType:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        if re.search(r'\d+(?:\.\d+)?%', claim):
            return ClaimType.STATISTICAL
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–∞—Ç—ã
        if re.search(r'\d{1,2}[./]\d{1,2}[./]\d{2,4}|\d{4}', claim):
            return ClaimType.TEMPORAL
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ü–∏—Ç–∞—Ç—ã
        if claim.startswith('"') and claim.endswith('"'):
            return ClaimType.QUOTE
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞—É—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        scientific_terms = ['–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', '—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç', '–∞–Ω–∞–ª–∏–∑', '–¥–∞–Ω–Ω—ã–µ', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã', '—É—á–µ–Ω—ã–µ', '–Ω–∞—É–∫–∞']
        if any(term in claim.lower() for term in scientific_terms):
            return ClaimType.SCIENTIFIC
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        historical_terms = ['–≤ –ø—Ä–æ—à–ª–æ–º', '–∏—Å—Ç–æ—Ä–∏—è', '–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–π', '–≤ 19', '–≤ 20']
        if any(term in claim.lower() for term in historical_terms):
            return ClaimType.HISTORICAL
        
        return ClaimType.GENERAL
    
    async def _verify_statistical_claim(self, claim: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        verification_sources = []
        evidence = []
        confidence_score = 0.0
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ Wikipedia
        if self.wikipedia_mcp:
            try:
                result = await self.wikipedia_mcp.search_statistics(claim)
                if result.success and result.data:
                    sources = result.data.get('sources', [])
                    verification_sources.extend(sources)
                    evidence.extend(result.data.get('evidence', []))
                    confidence_score += 0.4
                    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Wikipedia –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {len(sources)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ WikipediaMCP: {e}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if self.news_mcp:
            try:
                result = await self.news_mcp.execute_with_retry('get_news', query=claim, language='ru')
                if result.success and result.data:
                    articles = result.data.get('articles', [])
                    if articles:
                        verification_sources.extend([f"news_source_{i}" for i in range(min(3, len(articles)))])
                        evidence.extend([f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ"])
                        confidence_score += 0.3
                        logger.info(f"–ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ NewsMCP: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        if confidence_score >= 0.6:
            status = FactCheckStatus.VERIFIED
        elif confidence_score >= 0.3:
            status = FactCheckStatus.PARTIALLY_VERIFIED
        else:
            status = FactCheckStatus.UNVERIFIED
        
        return FactCheckResult(
            claim=claim,
            claim_type=ClaimType.STATISTICAL,
            status=status,
            confidence_score=min(confidence_score, 1.0),
            verification_sources=verification_sources,
            evidence=evidence,
            recommendations=self._generate_statistical_recommendations(claim, status)
        )
    
    async def _verify_temporal_claim(self, claim: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        verification_sources = []
        evidence = []
        confidence_score = 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ Wikipedia
        if self.wikipedia_mcp:
            try:
                result = await self.wikipedia_mcp.search_historical(claim)
                if result.success and result.data:
                    sources = result.data.get('sources', [])
                    verification_sources.extend(sources)
                    evidence.extend(result.data.get('evidence', []))
                    confidence_score += 0.5
                    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Wikipedia: {len(sources)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ WikipediaMCP: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if self.news_mcp:
            try:
                result = await self.news_mcp.execute_with_retry('get_news', query=claim, language='ru')
                if result.success and result.data:
                    articles = result.data.get('articles', [])
                    if articles:
                        verification_sources.extend([f"news_source_{i}" for i in range(min(3, len(articles)))])
                        evidence.extend([f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ"])
                        confidence_score += 0.3
                        logger.info(f"–ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ NewsMCP: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if confidence_score >= 0.7:
            status = FactCheckStatus.VERIFIED
        elif confidence_score >= 0.4:
            status = FactCheckStatus.PARTIALLY_VERIFIED
        else:
            status = FactCheckStatus.UNVERIFIED
        
        return FactCheckResult(
            claim=claim,
            claim_type=ClaimType.TEMPORAL,
            status=status,
            confidence_score=min(confidence_score, 1.0),
            verification_sources=verification_sources,
            evidence=evidence,
            recommendations=self._generate_temporal_recommendations(claim, status)
        )
    
    async def _verify_quote_claim(self, claim: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–∏—Ç–∞—Ç—É"""
        verification_sources = []
        evidence = []
        confidence_score = 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if self.news_mcp:
            try:
                result = await self.news_mcp.execute_with_retry('get_news', query=claim, language='ru')
                if result.success and result.data:
                    articles = result.data.get('articles', [])
                    if articles:
                        verification_sources.extend([f"news_source_{i}" for i in range(min(3, len(articles)))])
                        evidence.extend([f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Ü–∏—Ç–∞—Ç–∞–º–∏"])
                        confidence_score += 0.6
                        logger.info(f"–ù–∞–π–¥–µ–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ü–∏—Ç–∞—Ç—ã: {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ NewsMCP: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ Wikipedia
        if self.wikipedia_mcp:
            try:
                result = await self.wikipedia_mcp.search_general(claim)
                if result.success and result.data:
                    sources = result.data.get('sources', [])
                    verification_sources.extend(sources)
                    evidence.extend(result.data.get('evidence', []))
                    confidence_score += 0.3
                    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Wikipedia –¥–ª—è —Ü–∏—Ç–∞—Ç—ã: {len(sources)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ WikipediaMCP: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if confidence_score >= 0.7:
            status = FactCheckStatus.VERIFIED
        elif confidence_score >= 0.4:
            status = FactCheckStatus.PARTIALLY_VERIFIED
        else:
            status = FactCheckStatus.UNVERIFIED
        
        return FactCheckResult(
            claim=claim,
            claim_type=ClaimType.QUOTE,
            status=status,
            confidence_score=min(confidence_score, 1.0),
            verification_sources=verification_sources,
            evidence=evidence,
            recommendations=self._generate_quote_recommendations(claim, status)
        )
    
    async def _verify_scientific_claim(self, claim: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞—É—á–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        verification_sources = []
        evidence = []
        confidence_score = 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ Wikipedia
        if self.wikipedia_mcp:
            try:
                result = await self.wikipedia_mcp.search_scientific(claim)
                if result.success and result.data:
                    sources = result.data.get('sources', [])
                    verification_sources.extend(sources)
                    evidence.extend(result.data.get('evidence', []))
                    confidence_score += 0.5
                    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –Ω–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Wikipedia: {len(sources)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ WikipediaMCP: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if self.news_mcp:
            try:
                result = await self.news_mcp.execute_with_retry('get_news', query=claim, language='ru')
                if result.success and result.data:
                    articles = result.data.get('articles', [])
                    if articles:
                        verification_sources.extend([f"news_source_{i}" for i in range(min(3, len(articles)))])
                        evidence.extend([f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ –Ω–∞—É—á–Ω–æ–π —Ç–µ–º–µ"])
                        confidence_score += 0.3
                        logger.info(f"–ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –Ω–∞—É—á–Ω–æ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ NewsMCP: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if confidence_score >= 0.6:
            status = FactCheckStatus.VERIFIED
        elif confidence_score >= 0.3:
            status = FactCheckStatus.PARTIALLY_VERIFIED
        else:
            status = FactCheckStatus.UNVERIFIED
        
        return FactCheckResult(
            claim=claim,
            claim_type=ClaimType.SCIENTIFIC,
            status=status,
            confidence_score=min(confidence_score, 1.0),
            verification_sources=verification_sources,
            evidence=evidence,
            recommendations=self._generate_scientific_recommendations(claim, status)
        )
    
    async def _verify_general_claim(self, claim: str) -> FactCheckResult:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—â–µ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ"""
        verification_sources = []
        evidence = []
        confidence_score = 0.0
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ Wikipedia
        if self.wikipedia_mcp:
            try:
                result = await self.wikipedia_mcp.search_general(claim)
                if result.success and result.data:
                    sources = result.data.get('sources', [])
                    verification_sources.extend(sources)
                    evidence.extend(result.data.get('evidence', []))
                    confidence_score += 0.4
                    logger.info(f"–ù–∞–π–¥–µ–Ω—ã –æ–±—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ Wikipedia: {len(sources)}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ WikipediaMCP: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        if self.news_mcp:
            try:
                result = await self.news_mcp.execute_with_retry('get_news', query=claim, language='ru')
                if result.success and result.data:
                    articles = result.data.get('articles', [])
                    if articles:
                        verification_sources.extend([f"news_source_{i}" for i in range(min(3, len(articles)))])
                        evidence.extend([f"–ù–∞–π–¥–µ–Ω–æ {len(articles)} –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ —Ç–µ–º–µ"])
                        confidence_score += 0.3
                        logger.info(f"–ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –æ–±—â–µ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {len(articles)} —Å—Ç–∞—Ç–µ–π")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ NewsMCP: {e}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        if confidence_score >= 0.6:
            status = FactCheckStatus.VERIFIED
        elif confidence_score >= 0.3:
            status = FactCheckStatus.PARTIALLY_VERIFIED
        else:
            status = FactCheckStatus.UNVERIFIED
        
        return FactCheckResult(
            claim=claim,
            claim_type=ClaimType.GENERAL,
            status=status,
            confidence_score=min(confidence_score, 1.0),
            verification_sources=verification_sources,
            evidence=evidence,
            recommendations=self._generate_general_recommendations(claim, status)
        )
    
    def _generate_statistical_recommendations(self, claim: str, status: FactCheckStatus) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
        recommendations = []
        
        if status == FactCheckStatus.UNVERIFIED:
            recommendations.append("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫")
        elif status == FactCheckStatus.PARTIALLY_VERIFIED:
            recommendations.append("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        elif status == FactCheckStatus.VERIFIED:
            recommendations.append("–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        
        return recommendations
    
    def _generate_temporal_recommendations(self, claim: str, status: FactCheckStatus) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
        recommendations = []
        
        if status == FactCheckStatus.UNVERIFIED:
            recommendations.append("–í—Ä–µ–º–µ–Ω–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞—Ç—É")
        elif status == FactCheckStatus.PARTIALLY_VERIFIED:
            recommendations.append("–î–∞—Ç–∞ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏")
        elif status == FactCheckStatus.VERIFIED:
            recommendations.append("–í—Ä–µ–º–µ–Ω–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        
        return recommendations
    
    def _generate_quote_recommendations(self, claim: str, status: FactCheckStatus) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ü–∏—Ç–∞—Ç"""
        recommendations = []
        
        if status == FactCheckStatus.UNVERIFIED:
            recommendations.append("–¶–∏—Ç–∞—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É—Ç–æ—á–Ω–∏—Ç—å –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ")
        elif status == FactCheckStatus.PARTIALLY_VERIFIED:
            recommendations.append("–¶–∏—Ç–∞—Ç–∞ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç")
        elif status == FactCheckStatus.VERIFIED:
            recommendations.append("–¶–∏—Ç–∞—Ç–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –∞–≤—Ç–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        
        return recommendations
    
    def _generate_scientific_recommendations(self, claim: str, status: FactCheckStatus) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
        recommendations = []
        
        if status == FactCheckStatus.UNVERIFIED:
            recommendations.append("–ù–∞—É—á–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö")
        elif status == FactCheckStatus.PARTIALLY_VERIFIED:
            recommendations.append("–ù–∞—É—á–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
        elif status == FactCheckStatus.VERIFIED:
            recommendations.append("–ù–∞—É—á–Ω–æ–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ —Ä–µ—Ü–µ–Ω–∑–∏—Ä—É–µ–º—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        
        return recommendations
    
    def _generate_general_recommendations(self, claim: str, status: FactCheckStatus) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–±—â–∏—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π"""
        recommendations = []
        
        if status == FactCheckStatus.UNVERIFIED:
            recommendations.append("–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        elif status == FactCheckStatus.PARTIALLY_VERIFIED:
            recommendations.append("–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        elif status == FactCheckStatus.VERIFIED:
            recommendations.append("–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –Ω–∞–¥–µ–∂–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
        
        return recommendations
    
    async def _generate_recommendations(self, fact_check_results: List[FactCheckResult]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        unverified_count = sum(1 for r in fact_check_results 
                             if r.status == FactCheckStatus.UNVERIFIED)
        disputed_count = sum(1 for r in fact_check_results 
                           if r.status == FactCheckStatus.DISPUTED)
        false_count = sum(1 for r in fact_check_results 
                        if r.status == FactCheckStatus.FALSE)
        
        if false_count > 0:
            recommendations.append(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {false_count} –ª–æ–∂–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π - —Ç—Ä–µ–±—É–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ")
        
        if disputed_count > 0:
            recommendations.append(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {disputed_count} —Å–ø–æ—Ä–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–º–æ—Ç—Ä")
        
        if unverified_count > 0:
            recommendations.append(f"‚ÑπÔ∏è {unverified_count} —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if len(fact_check_results) > 0:
            avg_confidence = sum(r.confidence_score for r in fact_check_results) / len(fact_check_results)
            if avg_confidence < 0.5:
                recommendations.append("üìä –ù–∏–∑–∫–∞—è –æ–±—â–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
            elif avg_confidence > 0.8:
                recommendations.append("‚úÖ –í—ã—Å–æ–∫–∞—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –≥–æ—Ç–æ–≤ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            else:
                recommendations.append("üìù –°—Ä–µ–¥–Ω—è—è –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–ø–æ—Ä–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")
        
        return recommendations
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
        return {
            "cached_facts": len(self.fact_cache),
            "cache_ttl_hours": self.cache_ttl.total_seconds() / 3600,
            "oldest_cached": min(
                (result.checked_at for result in self.fact_cache.values()),
                default=None
            ).isoformat() if self.fact_cache else None
        }
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤"""
        self.fact_cache.clear()
        logger.info("–ö—ç—à –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤ –æ—á–∏—â–µ–Ω")
