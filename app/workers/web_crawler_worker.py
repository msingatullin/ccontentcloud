"""
Web Crawler Worker –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import logging
import threading
import time
import asyncio
from datetime import datetime
from typing import Dict, List, Optional, Any
import requests
from openai import AsyncOpenAI
import os

from app.services.content_source_service import ContentSourceService, MonitoredItemService, SourceCheckHistoryService
from app.services.content_extractor import ContentExtractor, ChangeDetector, RSSParser
from app.services.scheduled_post_service import ScheduledPostService
from app.services.production_calendar_service import ProductionCalendarService
from app.database.connection import get_db_session
from app.models.content_sources import ContentSource

logger = logging.getLogger(__name__)


class WebCrawlerWorker:
    """Worker –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, check_interval: int = 60):
        """
        Args:
            check_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 60)
        """
        self.check_interval = check_interval
        self.running = False
        self.thread = None
        self.openai_client = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º OpenAI –∫–ª–∏–µ–Ω—Ç
        api_key = os.environ.get('OPENAI_API_KEY')
        if api_key:
            self.openai_client = AsyncOpenAI(api_key=api_key)
        else:
            logger.warning("OPENAI_API_KEY not set, AI extraction will be disabled")
        
        self.content_extractor = ContentExtractor(self.openai_client)
        self.change_detector = ChangeDetector()
        
        logger.info(f"WebCrawlerWorker initialized with check_interval={check_interval}s")
    
    def start(self):
        """–ó–∞–ø—É—Å–∫ worker –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        if self.running:
            logger.warning("WebCrawlerWorker already running")
            return
        
        self.running = True
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        logger.info("WebCrawlerWorker started")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ worker"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=10)
        logger.info("WebCrawlerWorker stopped")
    
    def _run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª worker"""
        logger.info("WebCrawlerWorker main loop started")
        
        while self.running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                sources = ContentSourceService.get_sources_to_check(limit=10)
                
                if sources:
                    logger.info(f"Found {len(sources)} sources to check")
                    
                    for source in sources:
                        if not self.running:
                            break
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                        asyncio.run(self._check_source(source))
                
                # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–µ–π
                time.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"Error in WebCrawlerWorker main loop: {e}", exc_info=True)
                time.sleep(self.check_interval)
    
    async def _check_source(self, source):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        start_time = time.time()
        logger.info(f"Checking source: {source.id} - {source.name} ({source.source_type})")
        
        try:
            items_found = 0
            items_new = 0
            items_duplicate = 0
            items_posted = 0
            
            # –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥
            if source.source_type == 'rss':
                # –ü—Ä—è–º–æ–π RSS - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –µ—Å—Ç—å
                result = await self._check_rss_source(source)
            elif source.source_type == 'website' or source.source_type == 'auto':
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ RSS –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
                result = await self._check_source_smart(source)
            else:
                logger.warning(f"Unsupported source type: {source.source_type}, trying smart detection")
                result = await self._check_source_smart(source)
            
            items_found = result.get('items_found', 0)
            items_new = result.get('items_new', 0)
            items_duplicate = result.get('items_duplicate', 0)
            items_posted = result.get('items_posted', 0)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            ContentSourceService.update_check_status(
                source.id,
                status='success',
                items_found=items_found,
                items_new=items_new
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø—Ä–æ–≤–µ—Ä–∫–∏
            execution_time = int((time.time() - start_time) * 1000)
            SourceCheckHistoryService.create_history(
                source_id=source.id,
                items_found=items_found,
                items_new=items_new,
                items_duplicate=items_duplicate,
                items_posted=items_posted,
                status='success',
                execution_time_ms=execution_time
            )
            
            logger.info(f"Source {source.id} checked successfully: {items_new} new items, {items_posted} posts created")
            
            return {
                'items_found': items_found,
                'items_new': items_new,
                'items_duplicate': items_duplicate,
                'items_posted': items_posted
            }
            
        except Exception as e:
            logger.error(f"Error checking source {source.id}: {e}", exc_info=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å, –Ω–æ –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–º–µ—á–∞–µ–º –∫–∞–∫ "–Ω–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"
            try:
                ContentSourceService.update_check_status(
                    source.id,
                    status='success',  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—Ö, —á—Ç–æ–±—ã –Ω–µ –ø—É–≥–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    items_found=0,
                    items_new=0
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
                execution_time = int((time.time() - start_time) * 1000)
                SourceCheckHistoryService.create_history(
                    source_id=source.id,
                    items_found=0,
                    items_new=0,
                    items_duplicate=0,
                    items_posted=0,
                    status='success',  # –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                    execution_time_ms=execution_time
                )
            except Exception as update_error:
                logger.error(f"Error updating status after error: {update_error}")
    
    async def _check_rss_source(self, source) -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        items_found = 0
        items_new = 0
        items_duplicate = 0
        items_posted = 0
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º RSS –ª–µ–Ω—Ç—É
            response = requests.get(source.url, timeout=30)
            response.raise_for_status()
            
            # –ü–∞—Ä—Å–∏–º RSS
            feed_items = RSSParser.parse_feed(response.text)
            items_found = len(feed_items)
            
            logger.info(f"RSS source {source.id}: found {items_found} items")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
            for feed_item in feed_items[:20]:  # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º 20 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
                external_id = feed_item.get('external_id')
                url = feed_item.get('url')
                
                duplicate = MonitoredItemService.check_duplicate(source.id, external_id, url)
                if duplicate:
                    items_duplicate += 1
                    continue
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if not self._matches_filters(feed_item, source):
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                monitored_item = MonitoredItemService.create_item(
                    source_id=source.id,
                    user_id=source.user_id,
                    title=feed_item.get('title', 'Untitled'),
                    content=feed_item.get('content', ''),
                    summary=feed_item.get('summary', ''),
                    url=url,
                    author=feed_item.get('author'),
                    external_id=external_id,
                    status='new',
                    raw_data=feed_item,
                    relevance_score=0.7  # –ë–∞–∑–æ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è RSS
                )
                
                if monitored_item:
                    items_new += 1
                    
                    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥, —Å–æ–∑–¥–∞–µ–º –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ—Å—Ç
                    if source.auto_post_enabled:
                        posted = await self._create_scheduled_post(source, monitored_item, feed_item)
                        if posted:
                            items_posted += 1
            
            return {
                'items_found': items_found,
                'items_new': items_new,
                'items_duplicate': items_duplicate,
                'items_posted': items_posted
            }
            
        except Exception as e:
            logger.error(f"Error checking RSS source {source.id}: {e}")
            raise
    
    async def _check_source_smart(self, source) -> Dict[str, int]:
        """
        –£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç RSS –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—Ä–∞—É–ª–µ—Ä
        –ù–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –ø—Ä–æ–±—É–µ—Ç –≤—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        """
        items_found = 0
        items_new = 0
        items_duplicate = 0
        items_posted = 0
        
        try:
            # 1. –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ RSS –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            logger.info(f"Smart check for source {source.id}: trying to discover RSS feed...")
            # discover_rss_feed –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ, –Ω–æ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ try-except –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            try:
                rss_url = RSSParser.discover_rss_feed(source.url)
            except Exception as e:
                logger.warning(f"Error discovering RSS feed: {e}, will use crawler")
                rss_url = None
            
            if rss_url:
                logger.info(f"RSS feed discovered: {rss_url}, using RSS method")
                # –í—Ä–µ–º–µ–Ω–Ω–æ –º–µ–Ω—è–µ–º URL –Ω–∞ RSS –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                original_url = source.url
                try:
                    source.url = rss_url
                    result = await self._check_rss_source(source)
                    items_found = result.get('items_found', 0)
                    items_new = result.get('items_new', 0)
                    items_duplicate = result.get('items_duplicate', 0)
                    items_posted = result.get('items_posted', 0)
                    
                    # –ï—Å–ª–∏ RSS —Å—Ä–∞–±–æ—Ç–∞–ª - –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
                    if items_new > 0:
                        db = get_db_session()
                        try:
                            source_obj = db.query(ContentSource).filter(ContentSource.id == source.id).first()
                            if source_obj:
                                source_obj.source_type = 'rss'
                                source_obj.url = rss_url
                                db.commit()
                                logger.info(f"Updated source {source.id} to RSS type with URL {rss_url}")
                        finally:
                            db.close()
                    
                    return {
                        'items_found': items_found,
                        'items_new': items_new,
                        'items_duplicate': items_duplicate,
                        'items_posted': items_posted
                    }
                except Exception as e:
                    logger.warning(f"RSS method failed for {rss_url}, trying crawler: {e}")
                    source.url = original_url
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –∫—Ä–∞—É–ª–µ—Ä–æ–º
            
            # 2. –ï—Å–ª–∏ RSS –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–∞—É–ª–µ—Ä
            logger.info(f"Using crawler method for source {source.id}")
            result = await self._check_website_source(source)
            return result
            
        except Exception as e:
            logger.error(f"Error in smart check for source {source.id}: {e}")
            # –ù–µ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –æ—à–∏–±–∫—É –¥–∞–ª—å—à–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            return {
                'items_found': 0,
                'items_new': 0,
                'items_duplicate': 0,
                'items_posted': 0
            }
    
    async def _check_website_source(self, source) -> Dict[str, int]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ website –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –ø–æ–º–æ—â—å—é crawler"""
        items_found = 0
        items_new = 0
        items_duplicate = 0
        items_posted = 0
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            response = requests.get(source.url, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (compatible; ContentCurator/1.0; +https://content-curator.com)'
            })
            response.raise_for_status()
            html = response.text
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
            changes = self.change_detector.detect_changes(
                html,
                source.last_snapshot_data
            )
            
            if not changes.get('has_changes'):
                logger.info(f"Website source {source.id}: no changes detected")
                return {
                    'items_found': 0,
                    'items_new': 0,
                    'items_duplicate': 0,
                    'items_posted': 0
                }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ–≤—ã–π —Å–Ω–∏–º–æ–∫
            ContentSourceService.save_snapshot(
                source.id,
                changes.get('new_hash'),
                {'hash': changes.get('new_hash'), 'timestamp': datetime.utcnow().isoformat()}
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é AI
            extraction_hints = {
                'keywords': source.keywords,
                'categories': source.categories
            }
            
            extracted_data = await self.content_extractor.extract_from_html(
                html,
                source.url,
                extraction_hints
            )
            
            items_found = 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç –ø–æ URL
            duplicate = MonitoredItemService.check_duplicate(source.id, None, source.url)
            if duplicate:
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑–º–µ–Ω–∏–ª—Å—è
                MonitoredItemService.update_item_status(
                    duplicate.id,
                    status='new',
                    content=extracted_data.get('content', ''),
                    summary=extracted_data.get('summary', ''),
                    extracted_data=extracted_data,
                    relevance_score=extracted_data.get('relevance_score', 0.5)
                )
                items_duplicate += 1
            else:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                monitored_item = MonitoredItemService.create_item(
                    source_id=source.id,
                    user_id=source.user_id,
                    title=extracted_data.get('title', 'Untitled'),
                    content=extracted_data.get('content', ''),
                    summary=extracted_data.get('summary', ''),
                    url=source.url,
                    image_url=extracted_data.get('image_url'),
                    author=extracted_data.get('author'),
                    status='new',
                    extracted_data=extracted_data,
                    ai_summary=extracted_data.get('summary'),
                    ai_sentiment=extracted_data.get('sentiment'),
                    ai_category=extracted_data.get('category'),
                    ai_keywords=extracted_data.get('keywords', []),
                    relevance_score=extracted_data.get('relevance_score', 0.5)
                )
                
                if monitored_item:
                    items_new += 1
                    
                    # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥ –∏ —ç—Ç–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                    if source.auto_post_enabled and extracted_data.get('relevance_score', 0) >= 0.5:
                        posted = await self._create_scheduled_post(source, monitored_item, extracted_data)
                        if posted:
                            items_posted += 1
            
            return {
                'items_found': items_found,
                'items_new': items_new,
                'items_duplicate': items_duplicate,
                'items_posted': items_posted
            }
            
        except Exception as e:
            logger.error(f"Error checking website source {source.id}: {e}")
            raise
    
    def _matches_filters(self, item_data: Dict[str, Any], source) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —ç–ª–µ–º–µ–Ω—Ç–∞ —Ñ–∏–ª—å—Ç—Ä–∞–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ
        if not source.keywords and not source.exclude_keywords:
            return True
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        text = f"{item_data.get('title', '')} {item_data.get('summary', '')} {item_data.get('content', '')}".lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–∫–ª—é—á–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if source.exclude_keywords:
            for keyword in source.exclude_keywords:
                if keyword.lower() in text:
                    logger.debug(f"Item excluded by keyword: {keyword}")
                    return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∫–ª—é—á–∞—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if source.keywords:
            for keyword in source.keywords:
                if keyword.lower() in text:
                    return True
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –Ω–∏ –æ–¥–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            return False
        
        return True
    
    async def _create_scheduled_post(
        self,
        source,
        monitored_item,
        extracted_data: Dict[str, Any]
    ) -> bool:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø–æ—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω –ª–∏ –∞–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥
        if not source.auto_post_enabled:
            logger.debug(f"–ê–≤—Ç–æ–ø–æ—Å—Ç–∏–Ω–≥ –≤—ã–∫–ª—é—á–µ–Ω –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å—Ç–∞")
            return False
        
        db = get_db_session()
        try:
            # –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏
            from app.models.content import ContentPieceDB
            import uuid
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –ø–æ —à–∞–±–ª–æ–Ω—É
            if source.post_template:
                post_text = source.post_template.format(
                    title=extracted_data.get('title', ''),
                    description=extracted_data.get('summary', ''),
                    content=extracted_data.get('content', '')[:500],
                    url=extracted_data.get('url', ''),
                    author=extracted_data.get('author', '')
                )
            else:
                # –ë–∞–∑–æ–≤—ã–π —à–∞–±–ª–æ–Ω
                post_text = f"{extracted_data.get('title', '')}\n\n{extracted_data.get('summary', '')}"
                if extracted_data.get('url'):
                    post_text += f"\n\n{extracted_data['url']}"
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏
            content = ContentPieceDB(
                id=str(uuid.uuid4()),
                user_id=source.user_id,
                title=extracted_data.get('title', 'Untitled'),
                text=post_text,
                content_type='post',
                platform='telegram',
                status='draft',
                meta_data={
                    'source_id': source.id,
                    'monitored_item_id': monitored_item.id,
                    'url': extracted_data.get('url', ''),
                    'auto_generated': True,
                    'image_url': extracted_data.get('image_url')
                }
            )
            db.add(content)
            db.commit()
            db.refresh(content)
            logger.info(f"Created content piece {content.id} from monitored item {monitored_item.id}")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
            from datetime import timedelta
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è –∏–∑ config
            posting_schedule = source.config.get('posting_schedule', {}) if source.config else {}
            frequency = posting_schedule.get('frequency', '2h')
            forbidden_hours_start = posting_schedule.get('forbidden_hours_start', '22:00')
            forbidden_hours_end = posting_schedule.get('forbidden_hours_end', '08:00')
            weekends_mode = posting_schedule.get('weekends_mode', 'disabled')
            use_production_calendar = posting_schedule.get('use_production_calendar', True)
            weekends_schedule = posting_schedule.get('weekends_schedule', {})
            
            # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ –≤—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã
            base_time = datetime.utcnow()
            if frequency == '1h':
                scheduled_time = base_time + timedelta(hours=1)
            elif frequency == '2h':
                scheduled_time = base_time + timedelta(hours=2)
            elif frequency == '5h':
                scheduled_time = base_time + timedelta(hours=5)
            elif frequency == '1d':
                scheduled_time = base_time + timedelta(days=1)
            elif frequency == '2d':
                # –î–≤–∞ —Ä–∞–∑–∞ –≤ —Å—É—Ç–∫–∏ - —É—Ç—Ä–æ–º –∏ –≤–µ—á–µ—Ä–æ–º
                scheduled_time = base_time + timedelta(hours=12)
            else:
                scheduled_time = base_time + timedelta(minutes=source.post_delay_minutes or 0)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –ø–æ—Å—Ç–∏—Ç—å –≤ —ç—Ç–æ –≤—Ä–µ–º—è
            can_post = ProductionCalendarService.can_post_at_time(
                post_datetime=scheduled_time,
                forbidden_hours_start=forbidden_hours_start,
                forbidden_hours_end=forbidden_hours_end,
                weekends_mode=weekends_mode,
                use_production_calendar=use_production_calendar,
                weekends_schedule=weekends_schedule
            )
            
            # –ï—Å–ª–∏ –Ω–µ–ª—å–∑—è –ø–æ—Å—Ç–∏—Ç—å —Å–µ–π—á–∞—Å, –∏—â–µ–º –±–ª–∏–∂–∞–π—à–µ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è
            if not can_post:
                # –ü—Ä–æ–±—É–µ–º —Å–¥–≤–∏–Ω—É—Ç—å –≤—Ä–µ–º—è –≤–ø–µ—Ä–µ–¥ —Å —à–∞–≥–æ–º –≤ 1 —á–∞—Å, –º–∞–∫—Å–∏–º—É–º 24 —á–∞—Å–∞
                for hours_offset in range(1, 25):
                    candidate_time = scheduled_time + timedelta(hours=hours_offset)
                    if ProductionCalendarService.can_post_at_time(
                        post_datetime=candidate_time,
                        forbidden_hours_start=forbidden_hours_start,
                        forbidden_hours_end=forbidden_hours_end,
                        weekends_mode=weekends_mode,
                        use_production_calendar=use_production_calendar,
                        weekends_schedule=weekends_schedule
                    ):
                        scheduled_time = candidate_time
                        logger.info(f"Adjusted posting time for source {source.id} to {scheduled_time} due to schedule restrictions")
                        break
                else:
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–µ–µ –≤—Ä–µ–º—è –∑–∞ 24 —á–∞—Å–∞, –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º –Ω–∞ –∑–∞–≤—Ç—Ä–∞
                    scheduled_time = scheduled_time + timedelta(days=1)
                    scheduled_time = scheduled_time.replace(hour=9, minute=0, second=0, microsecond=0)
                    logger.warning(f"Could not find suitable posting time for source {source.id}, scheduled for {scheduled_time}")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π Telegram –∫–∞–Ω–∞–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            from app.models.telegram_channels import TelegramChannel
            telegram_channel = db.query(TelegramChannel).filter(
                TelegramChannel.user_id == source.user_id,
                TelegramChannel.is_active == True
            ).first()
            
            if not telegram_channel:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ Telegram –∫–∞–Ω–∞–ª–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {source.user_id}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.id}")
                logger.info(f"üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –î–æ–±–∞–≤—å—Ç–µ –∞–∫—Ç–∏–≤–Ω—ã–π Telegram –∫–∞–Ω–∞–ª –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö, —á—Ç–æ–±—ã –ø–æ—Å—Ç—ã —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                return False
            
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –∞–∫—Ç–∏–≤–Ω—ã–π Telegram –∫–∞–Ω–∞–ª {telegram_channel.id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {source.user_id}")
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–≤–∏—Å —Å —Å–µ—Å—Å–∏–µ–π –ë–î
            scheduled_service = ScheduledPostService(db)
            
            # –°–æ–∑–¥–∞–µ–º –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ—Å—Ç
            try:
                scheduled_post = scheduled_service.create_scheduled_post(
                    user_id=source.user_id,
                    content_id=content.id,
                    platform='telegram',
                    account_id=telegram_channel.id,
                    scheduled_time=scheduled_time,
                    publish_options={
                        'text': post_text,
                        'media_urls': [extracted_data.get('image_url')] if extracted_data.get('image_url') else None
                    }
                )
                
                if scheduled_post:
                    # –û–±–Ω–æ–≤–ª—è–µ–º monitored_item
                    MonitoredItemService.update_item_status(
                        monitored_item.id,
                        status='approved',
                        scheduled_post_id=scheduled_post.id
                    )
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–µ
                    source_obj = db.query(ContentSource).filter(ContentSource.id == source.id).first()
                    if source_obj:
                        source_obj.total_posts_created = (source_obj.total_posts_created or 0) + 1
                        db.commit()
                    
                    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –æ—Ç–ª–æ–∂–µ–Ω–Ω—ã–π –ø–æ—Å—Ç {scheduled_post.id} –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ '{extracted_data.get('title', '')[:50]}...' –¥–ª—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞ {source.id}")
                    logger.info(f"üìÖ –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω –Ω–∞ {scheduled_time}")
                    return True
                else:
                    logger.warning(f"Failed to create scheduled post for monitored item {monitored_item.id}")
                    return False
                    
            except Exception as e:
                logger.error(f"Error creating scheduled post via service: {e}", exc_info=True)
                return False
            
        except Exception as e:
            logger.error(f"Error creating scheduled post: {e}", exc_info=True)
            return False
        finally:
            db.close()

